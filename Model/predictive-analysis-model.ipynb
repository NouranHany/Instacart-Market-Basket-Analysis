{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prediciting your Future order!\n> This notebook is implementing a predictive analysis model, that predicts the products ordered in users' future order based on each purchasing history.","metadata":{}},{"cell_type":"markdown","source":"## Model Usage\n> **How this model will make better customer shopping expireince?**\n\n> Customers tend to do things quick and in an easy way. Thus can integrate this model with instacart's shopping software to initially autofill user's basket once openned the application.","metadata":{}},{"cell_type":"markdown","source":"# 1. Reading Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# garbage collector to free up memory\nimport gc\ngc.enable()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T21:23:15.106539Z","iopub.execute_input":"2022-05-27T21:23:15.106928Z","iopub.status.idle":"2022-05-27T21:23:15.136906Z","shell.execute_reply.started":"2022-05-27T21:23:15.106811Z","shell.execute_reply":"2022-05-27T21:23:15.136223Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"orders = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/orders.csv' )\norder_products_train = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/order_products__train.csv')\norder_products_prior = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/order_products__prior.csv')\nproducts = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/products.csv')\naisles = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/aisles.csv')\ndepartments = pd.read_csv('/kaggle/input/d/datasets/psparks/instacart-market-basket-analysis/departments.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:15.188701Z","iopub.execute_input":"2022-05-27T21:23:15.188909Z","iopub.status.idle":"2022-05-27T21:23:32.281257Z","shell.execute_reply.started":"2022-05-27T21:23:15.188884Z","shell.execute_reply":"2022-05-27T21:23:32.280344Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(train_data):\n    \n#  iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n    start_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n        end_mem = train_data.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:32.283139Z","iopub.execute_input":"2022-05-27T21:23:32.283394Z","iopub.status.idle":"2022-05-27T21:23:32.300811Z","shell.execute_reply.started":"2022-05-27T21:23:32.283361Z","shell.execute_reply":"2022-05-27T21:23:32.300014Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(order_products_prior)\nreduce_mem_usage(order_products_train)\nreduce_mem_usage(products)\nreduce_mem_usage(orders)\nreduce_mem_usage(departments)\nreduce_mem_usage(aisles)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:32.303798Z","iopub.execute_input":"2022-05-27T21:23:32.304114Z","iopub.status.idle":"2022-05-27T21:23:34.128359Z","shell.execute_reply.started":"2022-05-27T21:23:32.304085Z","shell.execute_reply":"2022-05-27T21:23:34.127624Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# get shape of each df\nprint(f\" aisles : {aisles.shape} \\n depts : {departments.shape} \\n order_prod_prior : {order_products_prior.shape} \\n order_products_train : {order_products_train.shape} \\n orders : {orders.shape} \\n products : {products.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:34.130502Z","iopub.execute_input":"2022-05-27T21:23:34.131886Z","iopub.status.idle":"2022-05-27T21:23:34.138347Z","shell.execute_reply.started":"2022-05-27T21:23:34.131850Z","shell.execute_reply":"2022-05-27T21:23:34.137487Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Helper functions to be able to correclty calculate the avergae of hours\n# The problem is that if we deal with hours and average them normally, the average of 1:00 and 23:00 will be 12 not 0:00\nimport datetime\nimport math\n\ndef datetime_to_radians(x):\n    # radians are calculated using a 24-hour circle, not 12-hour, starting at north and moving clockwise\n    seconds_from_midnight = 3600 * x\n    radians = float(seconds_from_midnight) / float(12 * 60 * 60) * 2.0 * math.pi\n    return radians\n\ndef average_angle(angles):\n    # angles measured in radians\n    x_sum = np.sum(np.sin(angles))\n    y_sum = np.sum(np.cos(angles))\n    x_mean = x_sum / float(len(angles))\n    y_mean = y_sum / float(len(angles))\n    return np.arctan2(x_mean, y_mean)\n\ndef radians_to_time_of_day(x):\n    # radians are measured clockwise from north and represent time in a 24-hour circle\n    seconds_from_midnight = int(float(x) / (2.0 * math.pi) * 12.0 * 60.0 * 60.0)\n    hour = seconds_from_midnight // 3600 % 24\n    minute = (seconds_from_midnight % 3600) // 60\n    second = seconds_from_midnight % 60\n    return datetime.time(hour, minute, second)\n    \ndef average_times_of_day(x):\n    # input datetime.datetime array and output datetime.time value\n    angles = [datetime_to_radians(y) for y in x]\n    avg_angle = average_angle(angles)\n    return radians_to_time_of_day(avg_angle)\n\ndef day_to_radians(day):\n    radians = float(day) / float(7) * 2.0 * math.pi\n    return radians\ndef radians_to_days(x):\n    day = int(float(x) / (2.0 * math.pi) * 7) % 7\n    return day\ndef average_days(x):\n    angles = [day_to_radians(y) for y in x]\n    avg_angle = average_angle(angles)\n    return radians_to_days(avg_angle)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:34.140422Z","iopub.execute_input":"2022-05-27T21:23:34.140795Z","iopub.status.idle":"2022-05-27T21:23:34.154024Z","shell.execute_reply.started":"2022-05-27T21:23:34.140758Z","shell.execute_reply":"2022-05-27T21:23:34.153147Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 2. Predictor Features","metadata":{}},{"cell_type":"markdown","source":"### 2.1 User predictors\n","metadata":{}},{"cell_type":"markdown","source":"#### We start by calculating three new variables:\n\nuser_orders: Total number of orders per user\n\nuser_period: The time period (in days) between the first and last order of a user\n\nuser_mean_days_since_prior: Mean time period (in days) between two consequtive orders of a user","metadata":{}},{"cell_type":"code","source":"# We keep only the prior orders\nusers = orders[orders['eval_set'] == 'prior']\nusers['days_since_prior_order'].dropna()\n\n# We group orders by user_id & calculate the variables based on different user_id\nusers = users.groupby('user_id').agg(\n    \n user_orders= ('order_number' , max),\n user_period=('days_since_prior_order', sum),\n user_mean_days_since_prior = ('days_since_prior_order','mean')\n    \n)\nusers.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:34.155288Z","iopub.execute_input":"2022-05-27T21:23:34.155631Z","iopub.status.idle":"2022-05-27T21:23:34.486280Z","shell.execute_reply.started":"2022-05-27T21:23:34.155595Z","shell.execute_reply":"2022-05-27T21:23:34.485550Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### calculate three more new variables:\n\nuser_total_products: Total numbers of basket items included in user's orders\n\nuser_reorder_ratio: Ratio a user orders reorordered products.\n\nuser_distinct_products: Total number of distinct products ordered by a user","metadata":{}},{"cell_type":"code","source":"# We create a new table \"orders_products\" which contains the tables \"orders\" and \"order_products_prior\"\norders_products =pd.merge(orders , order_products_prior, on='order_id', how='inner')\n\n# Getting the number of products in each basket(order)\ngroupedorders_products = orders_products.groupby(['order_id']).agg(\n    basket_size = ('product_id', 'count')\n).reset_index()\norders_products = orders_products.merge(groupedorders_products, on='order_id', how='left')\norders_products.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:34.487520Z","iopub.execute_input":"2022-05-27T21:23:34.487774Z","iopub.status.idle":"2022-05-27T21:23:48.656488Z","shell.execute_reply.started":"2022-05-27T21:23:34.487739Z","shell.execute_reply":"2022-05-27T21:23:48.655651Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"orders_products['p_reordered']= orders_products['reordered']==1\norders_products['non_first_order']= orders_products['order_number']>1\n\nus=orders_products\n\n# We group orders_products by user_id & calculate the variables based on different user_id\nus=orders_products.groupby('user_id').agg(\n    \n     user_total_products =('user_id','count') ,\n     p_reordered =('p_reordered', sum) ,\n     non_first_order =('non_first_order', sum),\n     user_distinct_products=('product_id','nunique')\n\n).reset_index()\n#    us['user_reorder_ratio'] = sum(reordered == 1) / sum(order_number > 1)\nus['user_reorder_ratio']=us['p_reordered']/us['non_first_order']\n\ndel us[\"p_reordered\"],us[\"non_first_order\"]\ndel orders_products['p_reordered' ],orders_products['non_first_order']\n\nus.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:48.658060Z","iopub.execute_input":"2022-05-27T21:23:48.658349Z","iopub.status.idle":"2022-05-27T21:23:58.814514Z","shell.execute_reply.started":"2022-05-27T21:23:48.658310Z","shell.execute_reply":"2022-05-27T21:23:58.813708Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Then we combine the users and us tables ussing inner_join() function and we calculate the final variable:\n\nuser_average_basket: Average number of basket items per order per user","metadata":{}},{"cell_type":"code","source":"users =pd.merge(users,us ,on='user_id',  how='inner')\n\n# We calculate the user_average_basket variable\nusers['user_average_basket'] = users['user_total_products'] / users['user_orders']\nusers.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:58.815642Z","iopub.execute_input":"2022-05-27T21:23:58.815907Z","iopub.status.idle":"2022-05-27T21:23:58.864106Z","shell.execute_reply.started":"2022-05-27T21:23:58.815869Z","shell.execute_reply":"2022-05-27T21:23:58.863259Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We now identify the future order per user and add them in the users table. The future orders are indicated as train and test in the eval_set variable. As a result, we will know what is the order_id of the future order per user, whether this order belongs in the train or test set, and the time in days since the last order.","metadata":{}},{"cell_type":"code","source":"# we exclude prior orders and thus we keep only train and test orders\nus = orders[orders['eval_set'] != 'prior']\nus['time_since_last_order'] = us['days_since_prior_order']\nus['future_order_dow'] = us['order_dow']\nus['future_order_hour_of_day'] = us['order_hour_of_day']\n\nus = us[['user_id','order_id','eval_set','time_since_last_order', 'future_order_dow', 'future_order_hour_of_day']]\n\n# We combine users and us tables and store the results into the users table\nusers_features = pd.merge(users , us, on='user_id', how='inner') \n\n# We delete the us table\ndel us, users\n\nusers_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:58.867364Z","iopub.execute_input":"2022-05-27T21:23:58.867639Z","iopub.status.idle":"2022-05-27T21:23:58.950611Z","shell.execute_reply.started":"2022-05-27T21:23:58.867611Z","shell.execute_reply":"2022-05-27T21:23:58.949889Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Product dependent Features\n* Total number of orders per product\n* Avg position in cart for the product\n* The position in cart mostly repeated for the product\n* Avg of the number of items that co-occur with this product\n* The number of items that mostly co-occur with this product","metadata":{}},{"cell_type":"code","source":"prod_features = orders_products.groupby(['product_id']).agg(\n    prod_freq = ('order_id', 'count'),\n    prod_avg_position = ('add_to_cart_order', 'mean')\n#     prod_avg_hour = ('order_hour_of_day', average_times_of_day),\n#     prod_avg_dow = ('order_dow', average_days)\n).reset_index()\n\nprod_features.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:23:58.951951Z","iopub.execute_input":"2022-05-27T21:23:58.952994Z","iopub.status.idle":"2022-05-27T21:24:00.056220Z","shell.execute_reply.started":"2022-05-27T21:23:58.952946Z","shell.execute_reply":"2022-05-27T21:24:00.055517Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* Probability a product is reordered after the first order → Fraction of people ordered at least one time.\n* In average how many times a product has been purchased by the users who purchased it at least once","metadata":{}},{"cell_type":"code","source":"non_first_order = orders_products['order_number'] != 1\n\ngroupedorders_products = orders_products[non_first_order].groupby(['product_id']).agg(\n    prod_reorder_ratio = ('reordered', 'mean')\n).reset_index()\n\nprod_features = prod_features.merge(groupedorders_products, on='product_id', how='left')\n\n# Group by users who have bought it more than once\n# get the count of orders each user bought having the product. \ngroupedorders_products = orders_products[non_first_order].groupby(['product_id', 'user_id']).agg(\n    user_prod_freq = ('order_id', 'count')\n).reset_index()\n\n# get the avg # of orders the user will buy having that product after buying it for the first time\ngroupedorders_products = groupedorders_products.groupby(['product_id']).agg(\n    user_prod_avg_freq = ('user_prod_freq', 'mean')\n).reset_index()\n\nprod_features = prod_features.merge(groupedorders_products, on='product_id', how='left')\ndel groupedorders_products, non_first_order\n\nprod_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:00.057575Z","iopub.execute_input":"2022-05-27T21:24:00.057990Z","iopub.status.idle":"2022-05-27T21:24:14.453465Z","shell.execute_reply.started":"2022-05-27T21:24:00.057951Z","shell.execute_reply":"2022-05-27T21:24:14.452712Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 User x Product Predictors \n\n##### We now create predictors that indicate how a user behaves towards a specific product. We store these predictors in the data table, which is also the final table that we create. Towards this end, we use both prd and users tables. \n##### We start by calculating four new variables:\nup_orders:The total times a user ordered a product\n\nup_first_order: What was the first time a user purchased a product\n\nup_last_order: What was the last time a user purchased a product\n\nup_average_cart_position: The average position in a user's cart of a product","metadata":{}},{"cell_type":"code","source":"# We create the data table starting from the orders_products table \ndata = orders_products\n\ndata = data.groupby(['user_id','product_id']).agg(\n\n up_orders= ('product_id', 'count'),\n up_first_order=('order_number', min),\n up_last_order = ('order_number',max),\n up_average_cart_position = ('add_to_cart_order','mean')\n#  up_avg_hour = ('order_hour_of_day', average_times_of_day),\n#  up_avg_dow = ('order_dow', average_days)\n).reset_index()\n \ndel orders_products\ndata.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:14.454897Z","iopub.execute_input":"2022-05-27T21:24:14.455161Z","iopub.status.idle":"2022-05-27T21:24:26.352655Z","shell.execute_reply.started":"2022-05-27T21:24:14.455126Z","shell.execute_reply":"2022-05-27T21:24:26.351748Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### Then We compine the data table with the prd and users tables and we calculate the final three variables.\n\nup_order_rate: Percentage of user’s orders that include a specific product\n\nup_orders_since_last_order: Number of orders since user’s last order of a product\n\nup_order_rate_since_first_order: Pecentage of orders since first order of a product in which a user purchased this product","metadata":{}},{"cell_type":"code","source":"\n# up_order_rate = up_orders / user_total_products\ndata = data.merge(users_features[['user_id','user_orders']], on='user_id' , how='left')\ndata['up_order_rate'] = data['up_orders']/data['user_orders']\n\n# up_orders_since_last_order = user_last_order - user_last_ordered_that_product\ndata['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n\n# From the moment the user know about the product, how frequent he then bought it in his next orders?\n# up_order_rate_since_first_order = up_orders / (user_orders - up_first_order + 1)\n# The + 1 is added since order_numbering starts from 1 not 0\ndata['up_order_rate_since_first_order'] = data['up_orders']/(data['user_orders'] - data['up_first_order'] + 1)\ndel data['user_orders']\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:26.354300Z","iopub.execute_input":"2022-05-27T21:24:26.354588Z","iopub.status.idle":"2022-05-27T21:24:28.087610Z","shell.execute_reply.started":"2022-05-27T21:24:26.354548Z","shell.execute_reply":"2022-05-27T21:24:28.086921Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"* It seems that user_id = 1 is always ordering product_id = 196 in all his orders","metadata":{}},{"cell_type":"markdown","source":"#### Merging product, user, product-user features","metadata":{}},{"cell_type":"code","source":"# Merging user and product features with the final features dataframe\ndata = data.merge(users_features, on='user_id', how='left').merge(prod_features, on='product_id', how='left')\ndel users_features, prod_features\n\norder_products_future = order_products_train.merge(orders, on='order_id', how='left')\norder_products_future = order_products_future[['user_id', 'product_id', 'reordered']]\ndata = data.merge(order_products_future, on=['user_id', 'product_id'], how='left')\n\n# Set 0 to Product who didn't exists in the future order so model can predict them as Not in future order.\ndata['reordered'].fillna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:28.089050Z","iopub.execute_input":"2022-05-27T21:24:28.089303Z","iopub.status.idle":"2022-05-27T21:24:43.384961Z","shell.execute_reply.started":"2022-05-27T21:24:28.089269Z","shell.execute_reply":"2022-05-27T21:24:43.384126Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Time-dependent Predictors\n#### 2.4.1 User-dependent\n- On avg the hour the user buys this product at - The future order hour.\n- On avg which day of week the user buys this product at - The future order's day of week.\n#### 2.4.2 product-dependent\n- On avg the hour the product is most bought at - The future order hour.\n- Frequency of buying this product at the future's order hour of day. ??\n- Frequency of buying this product on the future's order day of week. ??","metadata":{}},{"cell_type":"code","source":"'''\nCalculates the difference between 2 values from a looping sequence\ndist(X, Y) = min { X-Y, N-(X-Y) }\n'''\ndef diff_bet_time(arr1, arr2, max_value=23):\n    arr1 = pd.to_datetime(arr1, format='%H')\n    arr2 = pd.to_datetime(arr2, format='%H:%M:%S')\n    arr_diff = np.abs(arr1.dt.hour-arr2.dt.hour)\n    return np.minimum(arr_diff, max_value- (arr_diff-1))\n\n'''\nCalculates the difference between 2 values from a looping sequence\ndist(X, Y) = min { X-Y, N-(X-Y) }\n'''\ndef diff_bet_dow(arr1, arr2, max_value=6):\n    arr_diff = np.abs(arr1-arr2)\n    return np.minimum(arr_diff, max_value- (arr_diff-1))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:43.386380Z","iopub.execute_input":"2022-05-27T21:24:43.386652Z","iopub.status.idle":"2022-05-27T21:24:43.395539Z","shell.execute_reply.started":"2022-05-27T21:24:43.386616Z","shell.execute_reply":"2022-05-27T21:24:43.394843Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# data['up_hour_diff'] = diff_bet_time(data['future_order_hour_of_day'], data['up_avg_hour'])\n# data['up_dow_diff'] = diff_bet_dow(data['future_order_dow'], data['up_avg_dow'])\n\n# data['prod_hour_diff'] = diff_bet_time(data['future_order_hour_of_day'], data['prod_avg_hour'], )\n# data['prod_dow_diff'] = diff_bet_dow(data['prod_avg_dow'], data['future_order_dow'])\n\n# # del data['prod_avg_dow'], data['prod_avg_hour'], data['future_order_hour_of_day'], data['up_avg_hour'], data['future_order_dow'], data['up_avg_dow']","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:43.397037Z","iopub.execute_input":"2022-05-27T21:24:43.397517Z","iopub.status.idle":"2022-05-27T21:24:43.406353Z","shell.execute_reply.started":"2022-05-27T21:24:43.397467Z","shell.execute_reply":"2022-05-27T21:24:43.405567Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# # Saving features in a csv file\n# data.to_csv('./features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:43.408019Z","iopub.execute_input":"2022-05-27T21:24:43.408349Z","iopub.status.idle":"2022-05-27T21:24:43.415071Z","shell.execute_reply.started":"2022-05-27T21:24:43.408305Z","shell.execute_reply":"2022-05-27T21:24:43.414311Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Creating the X, y","metadata":{}},{"cell_type":"code","source":"# To saveup memory, delete any dataframe we won't use next\ndel order_products_prior, order_products_train, products, orders, departments, aisles","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:43.416180Z","iopub.execute_input":"2022-05-27T21:24:43.416978Z","iopub.status.idle":"2022-05-27T21:24:43.427649Z","shell.execute_reply.started":"2022-05-27T21:24:43.416941Z","shell.execute_reply":"2022-05-27T21:24:43.426952Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting data to train, valid and test\n- Note that we don't have y_test, since in the data given we don't know what products will be in the test future orders.\n- To have an direction on how our model performs, will create a validation set.\n- Train on train set, observe performance and tune parameters on valid set.\n","metadata":{}},{"cell_type":"code","source":"# Splitting data to train and test\nX_train = data[data['eval_set'] == 'train']\ny_train = X_train['reordered']\nX_test = data[data['eval_set'] == 'test']\ndel data","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:43.428788Z","iopub.execute_input":"2022-05-27T21:24:43.429627Z","iopub.status.idle":"2022-05-27T21:24:46.356302Z","shell.execute_reply.started":"2022-05-27T21:24:43.429589Z","shell.execute_reply":"2022-05-27T21:24:46.355530Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint('Class distribution before splitting')\npos_count = np.sum(X_train['reordered']==1)\nneg_count = np.sum(X_train['reordered']==0)\nprint('positive ratio: ', pos_count)\nprint('negative count: ', neg_count)\nprint('positive ratio: ', pos_count/(pos_count+neg_count))\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\nprint('Class distribution of Train set')\ntrain_pos_count = np.sum(X_train['reordered']==1)\ntrain_neg_count = np.sum(X_train['reordered']==0)\nprint('positive count: ', train_pos_count)\nprint('negative count: ', train_neg_count)\nprint('positive ratio: ', train_pos_count/(train_pos_count+train_neg_count))\n\nprint('Class distribution of Validation set')\nval_pos_count = np.sum(X_val['reordered']==1)\nval_neg_count = np.sum(X_val['reordered']==0)\nprint('positive count: ', val_pos_count)\nprint('negative count: ', val_neg_count)\nprint('positive ratio: ', val_pos_count/(val_pos_count+val_neg_count))\n\n# Removing eval_set and the target variable from features\nX_train_non_pred_vars = X_train[['product_id', 'order_id', 'user_id']]\nX_train.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\nX_val_non_pred_vars = X_val[['product_id', 'order_id', 'user_id']]\nX_val.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\nX_test_non_pred_vars = X_test[['product_id', 'order_id', 'user_id']]\nX_test.drop(['reordered', 'eval_set', 'product_id', 'order_id', 'user_id'], axis=1, inplace=True)\n\n# Drop features I suspect redundant or of no significant importance as the feature importance graph says\nX_train.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\nX_test.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\nX_val.drop(['up_orders', 'up_last_order', 'user_total_products', 'user_distinct_products'], axis=1, inplace=True)\n\n# Dropping the time dependent features 'up_dow_diff','prod_dow_diff','up_hour_diff','prod_hour_diff'\n# We had a strong intuition that these time features will be significant in the prediction,\n# However, they were the least feature's importance used by the model.\n# This is why We commented them across the code.","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:46.359468Z","iopub.execute_input":"2022-05-27T21:24:46.359702Z","iopub.status.idle":"2022-05-27T21:24:51.984778Z","shell.execute_reply.started":"2022-05-27T21:24:46.359675Z","shell.execute_reply":"2022-05-27T21:24:51.983996Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:51.986226Z","iopub.execute_input":"2022-05-27T21:24:51.986471Z","iopub.status.idle":"2022-05-27T21:24:51.993378Z","shell.execute_reply.started":"2022-05-27T21:24:51.986439Z","shell.execute_reply":"2022-05-27T21:24:51.992486Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:51.994575Z","iopub.execute_input":"2022-05-27T21:24:51.995133Z","iopub.status.idle":"2022-05-27T21:24:52.009598Z","shell.execute_reply.started":"2022-05-27T21:24:51.995092Z","shell.execute_reply":"2022-05-27T21:24:52.008779Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"markdown","source":"Notes from xgb documentation\n> * learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n> * max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n> * subsample: percentage of samples used per tree. Low value can lead to underfitting.\n> * colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n> * n_estimators: number of trees you want to build.\n> * objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\nXGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n> * gamma: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n> * alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n> * lambda: L2 regularization on leaf weights and is smoother than L1 regularization.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import metrics\n\n# Training the model with features except the product_id, user_id, order_id columns\nclf = xgb.XGBClassifier(objective='binary:logistic', colsample_bytree = 0.4, learning_rate = 0.1,\n                max_depth = 5, reg_lambda = 5.0, n_estimators = 100)\nclf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:24:52.019609Z","iopub.execute_input":"2022-05-27T21:24:52.019889Z","iopub.status.idle":"2022-05-27T21:37:35.514327Z","shell.execute_reply.started":"2022-05-27T21:24:52.019855Z","shell.execute_reply":"2022-05-27T21:37:35.513481Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectFromModel\n# Visualizing the Feature importance \nprint(clf.feature_importances_)\n\nxgb.plot_importance(clf)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:35.515937Z","iopub.execute_input":"2022-05-27T21:37:35.516227Z","iopub.status.idle":"2022-05-27T21:37:35.830517Z","shell.execute_reply.started":"2022-05-27T21:37:35.516190Z","shell.execute_reply":"2022-05-27T21:37:35.829844Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# keep probabilities for the positive outcome only\ny_test_prob = clf.predict_proba(X_test)[:, 1]\ny_val_prob = clf.predict_proba(X_val)[:, 1]\ny_train_prob = clf.predict_proba(X_train)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:35.832946Z","iopub.execute_input":"2022-05-27T21:37:35.833159Z","iopub.status.idle":"2022-05-27T21:37:54.937786Z","shell.execute_reply.started":"2022-05-27T21:37:35.833133Z","shell.execute_reply":"2022-05-27T21:37:54.936958Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_val, y_val_prob)\n# plot the precision-recall curves\nno_skill = len(y_val[y_val==1]) / len(y_val)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:54.941922Z","iopub.execute_input":"2022-05-27T21:37:54.942134Z","iopub.status.idle":"2022-05-27T21:37:58.740808Z","shell.execute_reply.started":"2022-05-27T21:37:54.942107Z","shell.execute_reply":"2022-05-27T21:37:58.739970Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"'''\nThis function maximizes a metric, while keeping another metric above a given threshold.\n'''\ndef maximize_metric_keep_metric(metric1_list, metric2_list, metric2_thresh=0.3):\n    for idx in range(len(metric1_list)):\n        if(metric2_list[idx] > metric2_thresh):\n            return idx\n    return -1","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:58.742404Z","iopub.execute_input":"2022-05-27T21:37:58.742687Z","iopub.status.idle":"2022-05-27T21:37:58.750073Z","shell.execute_reply.started":"2022-05-27T21:37:58.742649Z","shell.execute_reply":"2022-05-27T21:37:58.749310Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Choosing Threshold that maximizes the f1_score\nprecision, recall, thresholds = precision_recall_curve(y_val, y_val_prob)\nf1_scores = 2*recall*precision/(recall+precision)\nopt_indx = np.argmax(f1_scores)\nprint(\"Maximuim f1_score for the positive class: \", f1_scores[opt_indx])\nprint(\"Correspoding precision: \", precision[opt_indx])\nprint(\"Correspoding recall: \", recall[opt_indx])\nprint(\"Correspoding Threshold: \", thresholds[opt_indx])\nbest_thresh = thresholds[opt_indx]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:58.751156Z","iopub.execute_input":"2022-05-27T21:37:58.752006Z","iopub.status.idle":"2022-05-27T21:37:59.409866Z","shell.execute_reply.started":"2022-05-27T21:37:58.751948Z","shell.execute_reply":"2022-05-27T21:37:59.409056Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Choosing Threshold that maximizes recall, while keeping precision above 0.3\nopt_indx = maximize_metric_keep_metric(metric1_list=recall, metric2_list=precision, metric2_thresh=0.3)\nprint(\"Max recall for the positive class: \", recall[opt_indx])\nprint(\"Correspoding precision: \", precision[opt_indx])\nprint(\"Correspoding f1_score: \", f1_scores[opt_indx])\nprint(\"Correspoding Threshold: \", thresholds[opt_indx])\nbest_thresh = thresholds[opt_indx]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:59.411276Z","iopub.execute_input":"2022-05-27T21:37:59.411713Z","iopub.status.idle":"2022-05-27T21:37:59.955437Z","shell.execute_reply.started":"2022-05-27T21:37:59.411672Z","shell.execute_reply":"2022-05-27T21:37:59.954505Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Changing probabilities to crisp predicted values, useing the threshold obtained from the ROC-curve\ny_test_preds = y_test_prob>best_thresh\ny_val_preds = y_val_prob>best_thresh\ny_train_preds = y_train_prob>best_thresh","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:59.956838Z","iopub.execute_input":"2022-05-27T21:37:59.957095Z","iopub.status.idle":"2022-05-27T21:37:59.967634Z","shell.execute_reply.started":"2022-05-27T21:37:59.957060Z","shell.execute_reply":"2022-05-27T21:37:59.966918Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint('-----------------CLASSIFICATION REPORT--------------------')\nprint(\"Train positive class count: \", y_train.sum())\nprint(\"Train negative class count: \", y_train.shape[0] - y_train.sum())\nprint(\"Train Set tn, fp, fn, tp:\",confusion_matrix(y_train, y_train_preds).ravel())\nprint(\"Train Set report:\",classification_report(y_train, y_train_preds))\n\nprint(\"Validation positive class count: \", y_val.sum())\nprint(\"Validation negative class count: \", y_val.shape[0] - y_val.sum())\nprint(\"Validation Set tn, fp, fn, tp:\",confusion_matrix(y_val, y_val_preds).ravel())\nprint(\"Validation Set report:\",classification_report(y_val, y_val_preds))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:37:59.969055Z","iopub.execute_input":"2022-05-27T21:37:59.969752Z","iopub.status.idle":"2022-05-27T21:38:48.823916Z","shell.execute_reply.started":"2022-05-27T21:37:59.969711Z","shell.execute_reply":"2022-05-27T21:38:48.823115Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"> * By forming the X, and y in this way, data is sparse. It's very skewed to the negative class. Thus accuracy is not a good measure for how the model is performing.\n> * First, we've found that there's alot of false negatives, so we considered to decrease it, or increase the recall of the positive class.\n> * This means, we want to reduce the number of products the model say user won't predict in the future while he/she will actually does. On the other side, it's okay to allow some false positives, when the model recommends a products the user won't buy in the next order.\n> * Our aim is to change the threshold to maximize the recall, while keeping the precision above a certain threshold. \n> * Since skewed class distribution we will consider PR-curve not ROC-curve.","metadata":{}},{"cell_type":"markdown","source":"#### Preparing the sumission file","metadata":{}},{"cell_type":"code","source":"import csv\n\n# Append prediction to test_order details\ntest_orders = X_test_non_pred_vars[['order_id','product_id']]\ntest_orders['reordered'] = y_test_preds\n\n# Extracting orders who have no predicted products\nempty_orders = test_orders.groupby(['order_id']).agg(\n    count_reorders = ('reordered', 'sum')\n).reset_index()\nempty_orders = empty_orders[empty_orders['count_reorders'] == 0]\n\n# For orders who have predicted products \n# Extract the products predicted to be in the future order\ntest_orders = test_orders[test_orders['reordered'] == 1]\n# For each order group its predicted products together into a list \ntest_orders = test_orders.groupby('order_id')['product_id'].apply(list).reset_index(name='products')\n\n\ntest_orders.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:38:48.865941Z","iopub.execute_input":"2022-05-27T21:38:48.866767Z","iopub.status.idle":"2022-05-27T21:38:50.684477Z","shell.execute_reply.started":"2022-05-27T21:38:48.866721Z","shell.execute_reply":"2022-05-27T21:38:50.683688Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# csv header\nheaderNames = ['order_id', 'products']\nrows = []\n\nfor index, row in test_orders.iterrows():\n    products = ' '.join(str(product_id) for product_id in row['products']) \n    rows.append( \n        {'order_id': str(row['order_id']),\n        'products': products})\n\nfor index, row in empty_orders.iterrows():\n    rows.append( \n        {'order_id': str(row['order_id']),\n        'products': 'None'})\n    \nwith open('./submissions.csv', 'w', encoding='UTF-8', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=headerNames)\n    writer.writeheader()\n    writer.writerows(rows)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T21:38:50.685940Z","iopub.execute_input":"2022-05-27T21:38:50.686205Z","iopub.status.idle":"2022-05-27T21:38:55.766634Z","shell.execute_reply.started":"2022-05-27T21:38:50.686170Z","shell.execute_reply":"2022-05-27T21:38:55.765742Z"},"trusted":true},"execution_count":38,"outputs":[]}]}